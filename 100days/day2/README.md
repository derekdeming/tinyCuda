# Day 2: Vector Addition with CUDA

This example demonstrates how to perform elementwise vector addition on the GPU using CUDA. The goals of the implementation are:

1. **Allocate and transfer data** between host (CPU) and device (GPU) memory.
2. Perform **elementwise vector addition** on the GPU.
3. Include **basic error checks** for CUDA calls.
4. Verify correctness with a **CPU baseline**.

---

## Theory Overview

### CUDA Programming Model

- CUDA (Compute Unified Device Architecture) allows programming NVIDIA GPUs for general-purpose computations.
- A CUDA program typically consists of:
  - **Host code** running on the CPU.
  - **Kernel code** running on the GPU.

### GPU Parallelism

- The GPU runs code in many parallel **threads**.
- Threads are organized into **blocks**, and blocks form a **grid**.
- Each thread has a unique identifier you can use to compute its data index.

### Memory Transfers

- GPU memory is separate from CPU memory.
- We use `cudaMalloc` and `cudaFree` to allocate/free GPU memory.
- We use `cudaMemcpy` to copy data between host and device memory.

### Kernel Launch Syntax

- A kernel is called with `<<<gridSize, blockSize>>>`.
  - `blockSize` is the number of threads per block.
  - `gridSize` is the number of blocks in the grid.
- The product `gridSize * blockSize` must cover the entire data array length.

### Error Checking

- We use CUDA error checking macros to catch runtime errors.
- After a kernel launch, `cudaGetLastError()` captures any launch errors.
- `cudaDeviceSynchronize()` ensures the kernel has finished running, so subsequent errors can be detected.

---

> **Note:**
> i'm using the `o1` / `o1pro` to help write the below section by giving it my `.cu` file and asking it to generate the README docs. I will make sure to double check all information generated by `o1` / `o1pro` and provide my own edits as needed

## Accessory Items

### Headers

- `cuda_runtime.h` --> for CUDA api calls
- `vector` --> convenient host-side container for vectors
- `iostream` --> for input/output operations

### Macros

- `CHECK_CUDA_ERR(err)` --> macro for checking CUDA errors

### Kernel Function

- `__global__ void vecAddKernel(float* a, float* b, float* c, int n)` --> kernel for vector addition
- annotated with `__global__` to indicate it runs on the GPU
- each thread computes `idx = threadIdx.x + blockIdx.x * blockDim.x`
- if `idx` is within the array bounds, it adds corresponding elements from `a` and `b` and stores the result in `c`

### Host Function

- `void vecAddBaseline(float* a, float* b, float* c, int n)` --> host-side baseline for vector addition
- verifies the result by comparing the GPU output with the CPU baseline

### Main Function

- `int main()` --> main function for the program
- We initialize the host data in the `for(int i = 0; i < n; i++)` loop, allocate device memory with `cudaMalloc` and copy data from host to device with `cudaMemcpy`. We define the launch params (`blockSize` and `gridSize`) and launch the kernel with `<<<gridSize, blockSize>>>`.
- After launching the kernel, we wait for it to finish with `cudaDeviceSynchronize()`.
- We then verify the result with the CPU baseline.
- Finally, we free the allocated memory with `cudaFree`.

## Compile + Run

- `nvcc -o vecAdd vecAdd.cu`
- `./vecAdd`
- The program will output the result of the vector addition and verify it against the CPU baseline.

## Key takeaways 

- we need to explicitly transfer data between host and device memory. We can do this by using `cudaMalloc` and `cudaFree` for device memory, and `cudaMemcpy` for copying data.
- for launching the kernel we need to define the grid size and block size. The grid size is the number of blocks in the grid, and the block size is the number of threads per block.
- in order to index the threads we use this formula: `threadIdx.x + blockIdx.x * blockDim.x`.
- for large vector operations we can do large vector operations in parallel on the gpu
